Eugene M. Joseph  
emj2152

## Investigative Techniques
### Assignment 2: API Profile

# [Microsoft Azure's Computer Vision API](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)

Microsoft's Computer Vision API provides users access to endpoints where they can feed in local or remote image data and receive responses that process or classify the image data in an intelligent manner. For example, the API has endpoints for detecting faces, commercially branded content, adult content, and places of interest within an image. The API also has endpoints for describing the contents of an image in readable text and extracting typed and handwritten text and numbers from an image.

Users need to sign up for a free [Microsoft Azure](https://azure.microsoft.com/en-us/free/search/) account to begin using the API and this process requires verification via phone number and credit card. For reference, Microsoft Azure is a cloud computing service similar to Amazon’s AWS that provides a suite of tools for building and deploying cloud applications. Users can also access a trial of the API for seven days without verifying a credit card.

The first 5,000 API calls per month are free with a maximum rate limit of 20 calls per minute. Once that threshold is crossed, Microsoft offers variable pricing based on the region you access the services from and the number of API calls processed. For example, the East US region which is located in Virginia charges $1.50 per 1,000 API calls for the first one million calls. Additional information on their tiered pricing model can be accessed [here](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/).

Microsoft’s Computer Vision API could be a useful tool for journalists to quickly parse through large collections of images to extract high-level information like where the images were shot via landmarks, what people and objects are contained within them, and what text or numbers they contain.

## Authentication

All calls to the API must include either one of two keys called **Key 1** and **Key 2** generated by Microsoft Azure when you create a new Computer Vision instance under the Cognitive Services section of the portal. The chosen key is passed along to API requests under the header property `Ocp-Apim-Subscription-Key`.

Make sure this header property is properly configured in whatever platform you're using to make the requests.

It doesn't matter which key you choose. Both grant access to the full functionality of the API. Microsoft provides access to two keys so that users can temporarily share keys and expire and replace one key without experiencing any system downtime in production environments. For example, you can share one key with a colleague and once they've finished working with the API you can expire their key without it affecting your set up with the other key. Additional information on Microsoft's rationale behind having two keys can be found [here](https://www.microsoft.com/en-us/translator/business/faq/#development) and [here](https://blogs.msdn.microsoft.com/mast/2013/11/06/why-does-an-azure-storage-account-have-two-access-keys/).

With this in mind, we'll be using **Key 1** for the example queries provided below.

## Base URL

The base URL for accessing the Computer Vision API is
`https://{region}.api.cognitive.microsoft.com/vision/{version}/`  
where **region** is a placeholder for a short-code of the region you're accessing the API from and **version** is a placeholder for API version you're using.

For this tutorial we'll be using version 2.0 so our base URL will be: `https://{region}.api.cognitive.microsoft.com/vision/v2.0/`  
`
## API Region

When creating a Computer Vision instance on Azure, you need to specify a region through which you'll access the API endpoints from. A full list of regions supported can be accessed [here](https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa). A good practice is to choose a region close to where you're accessing the API from to minimize the latency of the requests.

 For this tutorial we'll be using the **East US** region located in Virginia. So our base endpoint to access the API will look like this: `https://eastus.api.cognitive.microsoft.com/vision/v2.0/`


## Sample Queries

### [Analyze Image](https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa)

The **analyze image** query is a **POST** request that takes in an image URL or a raw image binary through the request body and returns a JSON of image features detected. The image features detected can be fine-tuned using the  `visualParams` and `details` request parameters.

##### Request parameters

<u>visualFeatures</u> - A string containing the desired visual features separated by commas. Supported features include Categories, Tags, Description, Faces, Imagetype, Color, and Adult. In the example query below we specify a value of `adult, faces`. `faces` detects and returns a bounding box of all the faces on the image and also predicts the age of the individuals. `adult` predicts if the image contains pornographic or sexually suggestive content.

<u>details</u> - A string indicating whether to detect whether celebrities or landmarks are present in addition. In the example query below we specify a value of `celebrities` that identifies celebrities in the image.

<u>language</u> - A string indicating the language of results in the returned JSON. The default value is english.

Go [here](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa) for more information on these request parameter and their supported values.

##### Example Query
See *query_analyze_image.py* in this repo for access to Python code and a cURL to test this query out.
```
https://eastus.api.cognitive.microsoft.com/vision/v2.0/analyze?visualFeatures="adult, faces"
```

Request Body:
```
{
	"url":"https://fm.cnbc.com/applications/cnbc.com/resources/img/editorial/2018/07/16/105332385-1531754604468rtx6bmp1.530x298.jpg?v=1531754709"
}
```
![alt text](https://fm.cnbc.com/applications/cnbc.com/resources/img/editorial/2018/07/16/105332385-1531754604468rtx6bmp1.530x298.jpg?v=1531754709)

Return:  
The JSON returned below has detected that Donald Trump and Vladmir Putin are in the image with a high degree of confidence (> 0.99 for both) and has determined that the image does not contain any adult or racy content. The `faces` param that we passed in has returned the pixel values of the corners of the squares bounding the faces and has also predicted the ages of the faces.
```
{
    "categories": [
        {
            "name": "people_",
            "score": 0.47265625,
            "detail": {
                "celebrities": [
                    {
                        "name": "Vladimir Putin",
                        "confidence": 0.99776351451873779,
                        "faceRectangle": {
                            "left": 321,
                            "top": 86,
                            "width": 67,
                            "height": 67
                        }
                    },
                    {
                        "name": "Donald Trump",
                        "confidence": 0.98238706588745117,
                        "faceRectangle": {
                            "left": 90,
                            "top": 45,
                            "width": 66,
                            "height": 66
                        }
                    }
                ]
            }
        },
        {
            "name": "people_group",
            "score": 0.4609375,
            "detail": {
                "celebrities": [
                    {
                        "name": "Vladimir Putin",
                        "confidence": 0.99776351451873779,
                        "faceRectangle": {
                            "left": 321,
                            "top": 86,
                            "width": 67,
                            "height": 67
                        }
                    },
                    {
                        "name": "Donald Trump",
                        "confidence": 0.98238706588745117,
                        "faceRectangle": {
                            "left": 90,
                            "top": 45,
                            "width": 66,
                            "height": 66
                        }
                    }
                ]
            }
        }
    ],
    "adult": {
        "isAdultContent": false,
        "isRacyContent": false,
        "adultScore": 0.0056033506989479065,
        "racyScore": 0.00863671861588955
    },
    "faces": [
        {
            "age": 48,
            "gender": "Male",
            "faceRectangle": {
                "left": 321,
                "top": 86,
                "width": 67,
                "height": 67
            }
        },
        {
            "age": 71,
            "gender": "Male",
            "faceRectangle": {
                "left": 90,
                "top": 45,
                "width": 66,
                "height": 66
            }
        }
    ],
    "requestId": "e1b35fb9-7fd9-4a2a-98eb-92e5b7302aa6",
    "metadata": {
        "width": 530,
        "height": 298,
        "format": "Jpeg"
    }
}
```

### [Describe Image](https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe)

The **describe image** query is a **POST** request that takes in an image URL or a raw image binary through the request body and are returns a readable text description of the image along with a collection of content tags.

##### Example Query
See *query_describe_image_.py* in this repo for access to Python code and a cURL to test this query out.
```
'https://eastus.api.cognitive.microsoft.com/vision/v2.0/describe'
```

##### Request parameters

<u>maxCandidates</u> - Number of description sentences to return for the image.

Go [here](https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe) for more information on this request parameter and its supported values.

Request Body:
```
{
	"url":"https://journalism.columbia.edu/files/soj/styles/flex_full/public/content/image/2018/09/dsc_7432_0.jpg"
}
```
![alt text](https://journalism.columbia.edu/files/soj/styles/flex_full/public/content/image/2018/09/dsc_7432_0.jpg)



Return:  
The JSON returned below contains two text captions that describe the image in readable text. The `maxCandidate` param that we set to `2` ensured that we received two and not just one attempt at a descriptive sentence. We also received several text tags of features within the image.

```
{
    "description": {
        "tags": [
            "indoor",
            "laptop",
            "person",
            "sitting",
            "table",
            "computer",
            "window",
            "looking",
            "people",
            "woman",
            "front",
            "man",
            "using",
            "room",
            "desk",
            "young",
            "group",
            "living",
            "food"
        ],
        "captions": [
            {
                "text": "a group of people sitting at a table using a laptop computer",
                "confidence": 0.97835906241678683
            },
            {
                "text": "a group of people sitting at a table using a laptop",
                "confidence": 0.97735906241678683
            }
        ]
    },
    "requestId": "44bd2722-337d-4f25-8688-9a1dda4f2bcc",
    "metadata": {
        "width": 1030,
        "height": 500,
        "format": "Jpeg"
    }
}
```



### [OCR](https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fc)

The **OCR** query is a **POST** request that takes in an image URL or a raw image binary data through the request body and returns the words recognized in order with bounding box information.

##### Example Query
See *query_ocr.py* in this repo for access to Python code and a cURL to test this query out.
```
'https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr?language=en&detectOrientation=true'
```

##### Request parameters

<u>language</u> - The language code of the text to be detected. In the example query below. The default value is `unk` which autodetects the language. In the example query below we use `en` which sets the language to detect to english.

<u>detectOrientation</u> - A boolean to detect whether the image is oriented properly (not upside down or flipped) and corrects it if so. In the example query below we've set this property to true which allows the system to detect the orientation of the text and rotate accordingly to detect characters.

Go [here](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fc) for more information on these request parameters and their supported values.

##### Example Query
```
https://eastus.api.cognitive.microsoft.com/vision/v1.0/ocr?language=en&detectOrientation=true
```
Request Body:
```
{
	"url":"http://www.techmynd.com/wp-content/uploads/2009/11/upside-down.jpg"
}
```
![alt text](http://www.techmynd.com/wp-content/uploads/2009/11/upside-down.jpg)

Return:

The JSON returned below correctly detects that the image was upside down and lists the words in order with the pixel values of the bounding boxes around each word.
```
{
    "language": "en",
    "textAngle": 0,
    "orientation": "Down",
    "regions": [
        {
            "boundingBox": "22,29,463,72",
            "lines": [
                {
                    "boundingBox": "22,29,463,46",
                    "words": [
                        {
                            "boundingBox": "22,29,241,46",
                            "text": "UPSIDE"
                        },
                        {
                            "boundingBox": "284,29,201,46",
                            "text": "DOWN"
                        }
                    ]
                },
                {
                    "boundingBox": "60,85,396,16",
                    "words": [
                        {
                            "boundingBox": "60,85,61,16",
                            "text": "TURN"
                        },
                        {
                            "boundingBox": "126,85,61,16",
                            "text": "YOUR"
                        },
                        {
                            "boundingBox": "193,85,103,16",
                            "text": "MONITOR"
                        },
                        {
                            "boundingBox": "302,85,80,16",
                            "text": "UPSIDE"
                        },
                        {
                            "boundingBox": "389,85,67,16",
                            "text": "DOWN"
                        }
                    ]
                }
            ]
        }
    ]
}
```
